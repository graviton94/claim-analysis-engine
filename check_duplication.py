import pandas as pd
from pathlib import Path
import sys
import os

def verify_unique_claims(base_dir):
    """
    ì§€ì •ëœ ë””ë ‰í† ë¦¬ ë‚´ì˜ ëª¨ë“  parquet íŒŒì¼ì„ ìˆœíšŒí•˜ë©° ìƒë‹´ë²ˆí˜¸ ì¤‘ë³µì„ ê²€ì‚¬í•©ë‹ˆë‹¤.
    """
    # ê²½ë¡œ ì„¤ì • (ì ˆëŒ€ ê²½ë¡œ ë³€í™˜)
    root_path = Path(base_dir).resolve()
    print(f"ğŸ” ê²€ì¦ ì‹œì‘: {root_path} ë‚´ë¶€ì˜ íŒŒì¼€ì´ íŒŒì¼ì„ ìŠ¤ìº”í•©ë‹ˆë‹¤...")

    # ëª¨ë“  íŒŒì¼€ì´ íŒŒì¼ ì¬ê·€ì  íƒìƒ‰
    parquet_files = list(root_path.rglob("*.parquet"))
    
    if not parquet_files:
        print("âš ï¸ íŒŒì¼€ì´ íŒŒì¼(.parquet)ì„ í•˜ë‚˜ë„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        print(f"   ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”: {root_path}")
        return

    print(f"ğŸ“¦ ì´ {len(parquet_files)}ê°œì˜ íŒŒì¼ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ë°ì´í„° ë¡œë”© ì¤‘...")

    # ë°ì´í„° ìˆ˜ì§‘
    combined_data = []
    
    for file_path in parquet_files:
        try:
            # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„ ìœ„í•´ 'ìƒë‹´ë²ˆí˜¸' ì»¬ëŸ¼ë§Œ ë¡œë“œ
            df = pd.read_parquet(file_path, columns=['ìƒë‹´ë²ˆí˜¸'])
            
            # ì¶œì²˜ íŒŒì¼ ì¶”ì ì„ ìœ„í•´ ê²½ë¡œ ì •ë³´ ì¶”ê°€ (ì—°ë„/ì›”/íŒŒì¼ëª…)
            # ì˜ˆ: 2022/1/part-0.parquet
            rel_path = file_path.relative_to(root_path)
            df['source_file'] = str(rel_path)
            
            combined_data.append(df)
            
        except Exception as e:
            print(f"âŒ ì½ê¸° ì‹¤íŒ¨ ({file_path.name}): {e}")

    if not combined_data:
        print("âŒ ë¡œë“œëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ì „ì²´ ë°ì´í„° ë³‘í•©
    full_df = pd.concat(combined_data, ignore_index=True)
    
    # ì¤‘ë³µ ê²€ì‚¬ ë¡œì§
    total_count = len(full_df)
    unique_count = full_df['ìƒë‹´ë²ˆí˜¸'].nunique()
    duplicate_count = total_count - unique_count

    print("\n" + "="*40)
    print("ğŸ“Š [ê²€ì¦ ê²°ê³¼ ìš”ì•½]")
    print(f" - ì „ì²´ ë°ì´í„° ê±´ìˆ˜ : {total_count:,} ê±´")
    print(f" - ê³ ìœ  ìƒë‹´ë²ˆí˜¸ ìˆ˜ : {unique_count:,} ê±´")
    print(f" - ì¤‘ë³µëœ ìƒë‹´ë²ˆí˜¸ ìˆ˜ : {duplicate_count:,} ê±´")
    print("="*40 + "\n")

    if duplicate_count > 0:
        print("ğŸš¨ **ê²½ê³ : ì¤‘ë³µ ë°ì´í„°ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤!** ğŸš¨\n")
        
        # ì¤‘ë³µëœ ë°ì´í„° ì¶”ì¶œ (ëª¨ë“  ì¤‘ë³µ ì¸ìŠ¤í„´ìŠ¤ í¬í•¨)
        duplicates = full_df[full_df.duplicated('ìƒë‹´ë²ˆí˜¸', keep=False)].sort_values('ìƒë‹´ë²ˆí˜¸')
        
        # ì¤‘ë³µ ìƒì„¸ ë¦¬í¬íŠ¸
        dup_groups = duplicates.groupby('ìƒë‹´ë²ˆí˜¸')['source_file'].apply(list)
        
        print("ğŸ“„ [ì¤‘ë³µ ìƒì„¸ ë¦¬í¬íŠ¸]")
        for claim_id, sources in dup_groups.items():
            print(f"ğŸ”¸ ìƒë‹´ë²ˆí˜¸ [{claim_id}] ({len(sources)}íšŒ ì¤‘ë³µ):")
            for src in sources:
                print(f"   â””â”€ {src}")
            print("-" * 30)
    else:
        print("âœ… **ê²€ì¦ ì„±ê³µ: ëª¨ë“  ìƒë‹´ë²ˆí˜¸ê°€ ìœ ì¼í•©ë‹ˆë‹¤.** (ë¬´ê²°ì„± í™•ë³´)")

if __name__ == "__main__":
    # ê¸°ë³¸ ê²½ë¡œ ì„¤ì • (ì‚¬ìš©ì í™˜ê²½ì— ë§ì¶¤)
    default_path = r"data/hub"
    
    # í„°ë¯¸ë„ ì¸ìë¡œ ê²½ë¡œë¥¼ ë°›ê±°ë‚˜ ê¸°ë³¸ê°’ ì‚¬ìš©
    target_path = sys.argv[1] if len(sys.argv) > 1 else default_path
    
    if os.path.exists(target_path):
        verify_unique_claims(target_path)
    else:
        # ìœˆë„ìš° ì ˆëŒ€ ê²½ë¡œ ì˜ˆì‹œë¡œ ì¬ì‹œë„ (í˜¹ì‹œ ì‹¤í–‰ ìœ„ì¹˜ê°€ ë‹¤ë¥¼ ê²½ìš°ë¥¼ ëŒ€ë¹„)
        abs_path = r"C:\claim-analysis-engine\data\hub"
        if os.path.exists(abs_path):
            verify_unique_claims(abs_path)
        else:
            print(f"âŒ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {target_path}")
            print("   ì‚¬ìš©ë²•: python check_duplication.py [ë°ì´í„°í´ë”ê²½ë¡œ]")